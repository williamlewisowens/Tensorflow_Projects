{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n9.0\n15.0\n21.0\n27.0\n"
     ]
    }
   ],
   "source": [
    "x_vals = np.array([1., 3., 5., 7., 9.])\n",
    "x_data = tf.placeholder(tf.float32)\n",
    "m_const = tf.constant(3.)\n",
    "my_product = tf.multiply(x_data, m_const)\n",
    "for x_val in x_vals:\n",
    "    print(sess.run(my_product, feed_dict={x_data:x_val}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102.]\n [ 66.]\n [ 58.]]\n[[114.]\n [ 78.]\n [ 70.]]\n"
     ]
    }
   ],
   "source": [
    "# 3x5 array\n",
    "my_array = np.array([[1., 3., 5., 7., 9.],\n",
    "                     [-2., 0., 2., 4., 6.],\n",
    "                     [-6., -3., 0., 3., 6.]])\n",
    "\n",
    "# creates a second 3x5 array\n",
    "x_vals = np.array([my_array, my_array + 1])\n",
    "\n",
    "# Declare the tf placeholder\n",
    "# If we do not know the shape beforehand, use tf.placeholder(tf.float32, shape=(3,None))\n",
    "x_data = tf.placeholder(tf.float32, shape=(3, 5))\n",
    "\n",
    "# Create three constants, size 5x1 and two scalers\n",
    "m1 = tf.constant([[1.],[0.],[-1.],[2.],[4.]])\n",
    "m2 = tf.constant([[2.]])\n",
    "a1 = tf.constant([[10.]])\n",
    "\n",
    "# Declare the operations\n",
    "# matmul of 3x5 and 5x1, resulting in 3x1\n",
    "prod1 = tf.matmul(x_data,m1)\n",
    "\n",
    "# matmul of 3x1 and scaler, resulting in 3x1\n",
    "prod2 = tf.matmul(prod1,m2)\n",
    "\n",
    "# add of 3x1 and scaler\n",
    "add1 = tf.add(prod2,a1)\n",
    "\n",
    "for x_val in x_vals:\n",
    "    print(sess.run(add1, feed_dict={x_data:x_val}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of feeding in an image, 1 image, size 4x4, depth 1)\n",
    "x_shape = [1,4,4,1]\n",
    "x_val = np.random.uniform(size=x_shape)\n",
    "x_data = tf.placeholder(tf.float32,shape=x_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter = tf.constant(0.25, shape=[2,2,1,1])\n",
    "my_strides = [1,2,2,1]\n",
    "\n",
    "# image is 4x4, filter is 2x2, padding of zeros to keep it same size, stride of 2 makes the output 2x2\n",
    "mov_avg_layer = tf.nn.conv2d(x_data,my_filter,my_strides,padding='SAME',name='Moving_Avg_Window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input our resulting 2x2 image, multiply it by another 2x2, and then add 1s to it, basically ax+b\n",
    "def custom_layer(input_matrix):\n",
    "    input_matrix_squeezed = tf.squeeze(input_matrix)\n",
    "    # 2x2 constant\n",
    "    A = tf.constant([[1.,2.],[-1.,3.]])\n",
    "    B = tf.constant(1.,shape=[2,2])\n",
    "    temp1 = tf.matmul(A,input_matrix_squeezed)\n",
    "    temp = tf.add(temp1,B)\n",
    "    return tf.sigmoid(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We created a custom layer above, but we need to put this layer on the graph with a named scope\n",
    "# This makes it identifiable and collapsible/expandable on the computational graph\n",
    "with tf.name_scope('Custom_Layer') as scope:\n",
    "    custom_layer1 = custom_layer(mov_avg_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87691295 0.9499861 ]\n [0.813603   0.90669864]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(custom_layer1,feed_dict={x_data:x_val}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing loss function, page 35\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = tf.linspace(-1., 1., 500)\n",
    "target = tf.constant(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 norm loss, aka Euclidean loss function\n",
    "# Square of the distance to the target, very curved near target\n",
    "l2_y_vals = tf.square(target - x_vals)\n",
    "l2_y_out = sess.run(l2_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 norm loss, aka absolute loss function, take absolute value of distance to target\n",
    "# Better for outliers than L2, not as smooth near the target as L2 so may not converge as well\n",
    "l1_y_vals = tf.abs(target - x_vals)\n",
    "l1_y_out = sess.run(l1_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-Huber loss is continuous and smooth, attempts to take best part of L1 and L2 norms by being convex near target and less steep at extremes\n",
    "# Form depends on extra parameter delta, dictates how steep it will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification loss functions used to evaluate loss when predicting categorical outcomes\n",
    "# Redefine our predictions (x_vals) and target\n",
    "x_vals = tf.linspace(-3., 5., 500)\n",
    "target = tf.constant(1.)\n",
    "targets = tf.fill([500,], 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge loss mostly used with SVM, can be used in NN too. Computes loss between two target classes, 1 and -1\n",
    "hinge_y_vals = tf.maximum(0., 1. - tf.multiply(target, x_vals))\n",
    "hinge_y_out = sess.run(hinge_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss for binary case, aka logistic loss, predicting two classes 0 and 1\n",
    "# Measure distance from actual class (0 or 1) to the predicted value, usually a real number between 0 and 1\n",
    "xentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((1. -target), tf.log(1. - x_vals))\n",
    "xentropy_y_out = sess.run(xentropy_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid cross entropy loss, similar to cross entropy loss except we first transform x values by sigmoid function\n",
    "xentropy_sigmoid_y_vals = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_vals,labels=targets)\n",
    "xentropy_sigmoid_y_out = sess.run(xentropy_sigmoid_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted cross entropy loss is weighted version of sigmoid cross entropy loss, provide a weight on the positive target\n",
    "\n",
    "# Softmax cross-entropy loss operates on non-normalized outputs, measures loss when only one target category instead of multiple targets\n",
    "# Transforms outputs into probability distribution via softmax function, computes loss function from true probability distribution\n",
    "\n",
    "# Sparse softmax cross entropy loss is same as softmax cross entropy loss, but instead of target being probability distribution,\n",
    "# Index of which category is true, pass in the index of which category is the true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing back prop\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.random.normal(1, 0.1, 100)\n",
    "y_vals = np.repeat(10., 100)\n",
    "x_data = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(shape=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output = tf.multiply(x_data, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.square(my_output - y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = tf.train.GradientDescentOptimizer(learning_rate=0.02)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #25 A = [6.593229]\nLoss = [17.941595]\nStep #50 A = [8.602763]\nLoss = [0.4703996]\nStep #75 A = [9.335105]\nLoss = [1.6425915]\nStep #100 A = [9.634427]\nLoss = [0.00276154]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target:rand_y})\n",
    "    if (i+1)%25==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(loss, feed_dict={x_data:rand_x, y_target: rand_y})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3, 1, 50)))\n",
    "y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\n",
    "x_data = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(mean=10, shape=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output = tf.add(x_data, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output_expanded = tf.expand_dims(my_output, 0)\n",
    "y_target_expanded = tf.expand_dims(y_target, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output_expanded,labels=y_target_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = tf.train.GradientDescentOptimizer(0.05)\n",
    "train_step = my_opt.minimize(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #200 A = [-0.86611]\nLoss = [[0.24718349]]\nStep #400 A = [-0.8413897]\nLoss = [[0.08305367]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #600 A = [-0.775075]\nLoss = [[0.13139345]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #800 A = [-0.92235]\nLoss = [[0.65335244]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1000 A = [-0.8824818]\nLoss = [[0.18552333]]\nStep #1200 A = [-0.8412221]\nLoss = [[0.06006026]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1400 A = [-0.99667853]\nLoss = [[0.16939253]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1400):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target:rand_y})\n",
    "    if (i+1)%200==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(xentropy, feed_dict={x_data: rand_x, y_target: rand_y})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "x_vals = np.random.normal(1, 0.1, 100)\n",
    "y_vals = np.repeat(10., 100)\n",
    "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(shape=[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output = tf.matmul(x_data, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(my_output - y_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = tf.train.GradientDescentOptimizer(0.02)\n",
    "train_step = my_opt.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100 A = [[10.005499]]\nLoss = 0.84329927\n"
     ]
    }
   ],
   "source": [
    "loss_batch = []\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(100, size=batch_size)\n",
    "    rand_x = np.transpose([x_vals[rand_index]])\n",
    "    rand_y = np.transpose([y_vals[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    if (i+1)%100==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target : rand_y})\n",
    "        print('Loss = ' + str(temp_loss))\n",
    "        loss_batch.append(temp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
